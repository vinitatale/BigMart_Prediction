{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dython.nominal import associations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis,randint,uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d21a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV,train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Reading the data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Shape of training and test dataset\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2214d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Information of training data\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Statistical information of numerical column\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description of categorical column\n",
    "df_train.select_dtypes(include=['object', 'category']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculation of Age of outlet\n",
    "df_train['Outlet_Establishment_Year'] = 2025-df_train['Outlet_Establishment_Year']\n",
    "df_test['Outlet_Establishment_Year'] = 2025- df_test['Outlet_Establishment_Year']\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the percentage of missing values\n",
    "def calc_percent_NAs(df):\n",
    "    nans = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df), columns=['percent']) \n",
    "    idx = nans['percent'] > 0\n",
    "    return nans[idx]\n",
    "print('Training data missing data percentage \\n')\n",
    "print(calc_percent_NAs(df_train))\n",
    "print('\\nTest data missing data percentage \\n')\n",
    "print(calc_percent_NAs(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to analyse Numerical Column\n",
    "\n",
    "def analyze_numerical_columns(df):\n",
    "    # Select numeric columns\n",
    "    num_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    print(f\"\\n Numerical columns found: {list(num_cols)}\")\n",
    "    \n",
    "    for col in num_cols:\n",
    "        print(f\"\\n Analysis for: {col}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Skewness: {skew(df[col].dropna()):.2f}\")\n",
    "        print(f\"Kurtosis: {kurtosis(df[col].dropna()):.2f}\")\n",
    "        \n",
    "        # Distribution Plot\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=30)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Boxplot\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a19098",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Analysis of Numerical Columns\n",
    "analyze_numerical_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to analyse the categorical column\n",
    "def analyze_categorical_columns(df):\n",
    "    # Identify columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    print(f\"\\n Categorical columns: {list(categorical_cols)}\")\n",
    "    # Column to remove as unique item identifier are very high\n",
    "    remove_col = 'Item_Identifier' \n",
    "\n",
    "    # Remove the column if it exists\n",
    "    categorical_cols = [col for col in categorical_cols if col != remove_col]\n",
    "\n",
    "    for cat in categorical_cols:\n",
    "        print(f\"\\n Categorical Column: {cat}\")\n",
    "        print(df[cat].value_counts(dropna=False))\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.countplot(x=cat, data=df)\n",
    "        plt.title(f'Count of records by {cat}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        ##Categorical column Vs Sales\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.barplot(x=cat, y='Item_Outlet_Sales', data=df, estimator='mean')\n",
    "        plt.title(f'Average Item Outlet Sales by {cat}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Analysis of Categorical columns\n",
    "analyze_categorical_columns(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3643f",
   "metadata": {},
   "source": [
    "Imputation of Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0287ee",
   "metadata": {},
   "source": [
    "1. Outlet size: Outlet size is related to outlet information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate mode based on outlet type & outlet location type\n",
    "mode_per_group = (\n",
    "    df_train.groupby(['Outlet_Type', 'Outlet_Location_Type'])['Outlet_Size']\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "    .reset_index()\n",
    "    .rename(columns={'Outlet_Size': 'Mode_Outlet_Size'})\n",
    ")\n",
    "\n",
    "print(mode_per_group)\n",
    "#Insights: if Grocery Store then outlet size is Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imputation of Missing Value fpr Outlet size if Outlet Type Grocery Store,\n",
    "df_train.loc[(df_train['Outlet_Size'].isna()) & (df_train['Outlet_Type'] == 'Grocery Store'), 'Outlet_Size'] = 'Small'\n",
    "df_test.loc[(df_test['Outlet_Size'].isna()) & (df_test['Outlet_Type'] == 'Grocery Store'), 'Outlet_Size'] = 'Small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd0bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#else replace with mode of coombination of outlet type and output location type\n",
    "# Step 1: Calculate mode of Outlet Size for each group (Outlet_Type, output location type) remove grocery store\n",
    "mode_df = (\n",
    "    df_train[df_train.Outlet_Type!='Grocery Store'].groupby(['Outlet_Type', 'Outlet_Location_Type'])['Outlet_Size']\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "    .reset_index()\n",
    "    .rename(columns={'Outlet_Size': 'Outlet_Size_mode'})\n",
    ")\n",
    "#Step 2: Merge the mode back to original df\n",
    "df_train = df_train.merge(mode_df, on=['Outlet_Type', 'Outlet_Location_Type'], how='left')\n",
    "df_test = df_test.merge(mode_df, on=['Outlet_Type', 'Outlet_Location_Type'], how='left')\n",
    "\n",
    "df_train['Outlet_Size'] = np.where(df_train['Outlet_Size'].isna() & (df_train['Outlet_Type'] != 'Grocery Store'), df_train['Outlet_Size_mode'], df_train['Outlet_Size'])\n",
    "df_test['Outlet_Size'] = np.where(df_test['Outlet_Size'].isna() & (df_test['Outlet_Type'] != 'Grocery Store'), df_test['Outlet_Size_mode'], df_test['Outlet_Size'])\n",
    "\n",
    "df_train = df_train.drop('Outlet_Size_mode',axis=1)\n",
    "df_test = df_test.drop('Outlet_Size_mode',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49818d41",
   "metadata": {},
   "source": [
    "2. Item Weight: Item weight must be related to Item Identifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Relation between Item identifier and Item Weight (to verify if Item weight is not related to item identifier)\n",
    "df_new = df_train[df_train['Item_Weight'].notna()]\n",
    "unique_weights = df_new.groupby('Item_Identifier')['Item_Weight'].nunique()\n",
    "# Filter to show only those with more than 1 unique value\n",
    "inconsistent_weights = unique_weights[unique_weights > 1]\n",
    "\n",
    "# Display the result\n",
    "print(inconsistent_weights)\n",
    "\n",
    "#Insights: It shows there is one to one mapping between item identifier and item weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Impute the missing value of item weight with the corresponding item weight of identifier\n",
    "weight_map = df_train[df_train.Item_Weight.notna()].groupby('Item_Identifier')['Item_Weight'].mean()\n",
    "df_train['Item_Weight'] = df_train['Item_Weight'].fillna(df_train['Item_Identifier'].map(weight_map))\n",
    "df_test['Item_Weight'] = df_test['Item_Weight'].fillna(df_test['Item_Identifier'].map(weight_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d18afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_percent_NAs(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are few missing item weight of identifer for which weight is not available\n",
    "##Replace those missing values with median of item weight\n",
    "weight_map1 = df_train[df_train.Item_Weight.notna()].groupby('Item_Type')['Item_Weight'].mean()\n",
    "df_train['Item_Weight'] = df_train['Item_Weight'].fillna(df_train['Item_Type'].map(weight_map1))\n",
    "df_test['Item_Weight'] = df_test['Item_Weight'].fillna(df_test['Item_Type'].map(weight_map1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calc_percent_NAs(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadee422",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eca627",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculation of Item Visbility Mean Ratio\n",
    "# Group by Item_Identifier to calculate mean visibility\n",
    "item_visibility_avg = df_train.groupby('Item_Identifier')['Item_Visibility'].mean()\n",
    "\n",
    "df_train['Item_Visibility_Avg'] = df_train['Item_Identifier'].map(item_visibility_avg)\n",
    "df_train['Item_Visibility_MeanRatio'] = df_train['Item_Visibility'] / df_train['Item_Visibility_Avg']\n",
    "\n",
    "df_test['Item_Visibility_Avg'] = df_test['Item_Identifier'].map(item_visibility_avg)\n",
    "df_test['Item_Visibility_MeanRatio'] = df_test['Item_Visibility'] / df_test['Item_Visibility_Avg']\n",
    "\n",
    "df_train = df_train.drop('Item_Visibility_Avg',axis=1)\n",
    "df_test = df_test.drop('Item_Visibility_Avg',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e39967",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Regularize the value of Item Fat Content\n",
    "df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].replace({\n",
    "    'reg': 'Regular',\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat'\n",
    "})\n",
    "\n",
    "df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].replace({\n",
    "    'reg': 'Regular',\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correlation analysis\n",
    "fig,ax =plt.subplots(figsize = (8,8))\n",
    "heatmap = associations(df_train,num_num_assoc='spearman',nom_num_assoc='correlation_ratio',nom_nom_assoc='cramer',ax=ax,cmap='viridis')\n",
    "\n",
    "cbar=ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "cbar.ax.set_ylabel(\"Correlation\")\n",
    "cbar.ax.figure.axes[-1].set_aspect(20)\n",
    "cbar.ax.figure.colorbar(cbar.ax.collections[0],shrink=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categoricol column\n",
    "object=df_train.select_dtypes(include='object').columns\n",
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Outlet size is ordinal and there are only categories in Item Fat Content\n",
    "df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].map({'Regular':0,'Low Fat':1})\n",
    "df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].map({'Regular':0,'Low Fat':1})\n",
    "\n",
    "\n",
    "df_train['Outlet_Size'] = df_train['Outlet_Size'].map({'Small'  : 1,\n",
    "                                                 'Medium' : 2,\n",
    "                                                 'High'   : 3\n",
    "                                                 }).astype(int)\n",
    "\n",
    "df_test['Outlet_Size'] = df_test['Outlet_Size'].map({'Small'  : 1,\n",
    "                                                 'Medium' : 2,\n",
    "                                                 'High'   : 3\n",
    "                                                 }).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = df_train.copy()\n",
    "df_test1 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a71323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoder for Item identifer as there are many unique values \n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_train['Item_Identifier'] = encoder.fit_transform(df_train['Item_Identifier'])\n",
    "df_test['Item_Identifier'] = encoder.transform(df_test['Item_Identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd079f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehot encoding for remaining columns\n",
    "features_label = ['Item_Type','Outlet_Location_Type','Outlet_Identifier','Outlet_Type']\n",
    "df_train = pd.get_dummies(df_train, columns=features_label, drop_first=True)\n",
    "df_test  = pd.get_dummies(df_test,  columns=features_label, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99455caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop('Item_Outlet_Sales', axis=1)\n",
    "y = df_train['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed367a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training set and test set 80%-20%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b14219",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DecisionTreeRegressor(), RandomForestRegressor(), XGBRegressor(),LGBMRegressor()]\n",
    "\n",
    "for model in models:\n",
    "    reg = model\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(f'{model} MSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "RF_model = RandomForestRegressor(random_state=42)\n",
    "RF_model.fit(X_train, y_train)\n",
    "y_pred1 = RF_model.predict(X_test)\n",
    "# MSE\n",
    "mse = mean_squared_error(y_test, y_pred1)\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "# R²\n",
    "r2 = r2_score(y_test, y_pred1)\n",
    "# Print results\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23476ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RF_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel(\"Feature Importance (MSE Reduction)\")\n",
    "plt.title(\"Random Forest Regressor Feature Importances\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21057570",
   "metadata": {},
   "source": [
    "Hyperparameter tuning of Random Forest using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(400, 500),\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [5, 10,20],\n",
    "    'min_samples_leaf': [2, 4],\n",
    "    'max_features': ['sqrt', 'log2',None]\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65600086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Validation RMSE:\", rmse)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, color='dodgerblue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(f\"Actual vs Predicted (R² = {r2_score(y_test, y_pred):.2f})\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(random_state=42)\n",
    "\n",
    "LGB_model = model.fit(X_train, y_train)\n",
    "y_pred1 = LGB_model.predict(X_test)\n",
    "# MSE\n",
    "mse = mean_squared_error(y_test, y_pred1)\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "# R²\n",
    "r2 = r2_score(y_test, y_pred1)\n",
    "# Print results\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0975bc",
   "metadata": {},
   "source": [
    "Hyperparameter tuning of LGBM using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe540e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.05),\n",
    "    'n_estimators': randint(300, 500),\n",
    "    'min_child_samples': randint(10, 50),\n",
    "    'subsample': uniform(0.6, 0.4),  # 0.6 to 1.0\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0.0, 1.0),\n",
    "    'reg_lambda': uniform(0.0, 1.0)\n",
    "}\n",
    "\n",
    "model = LGBMRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of parameter settings sampled\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "best_model1 = random_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_model1.predict(X_test)\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, color='dodgerblue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(f\"Actual vs Predicted (R² = {r2_score(y_test, y_pred):.2f})\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc80e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction using Random Forest\n",
    "y_prediction = best_model.predict(df_test)\n",
    "df_pred = pd.DataFrame(y_prediction,columns=['Item_Outlet_Sales'])\n",
    "df_test2 = pd.read_csv('test.csv')\n",
    "df_test2 = df_test2[['Item_Identifier','Outlet_Identifier']]\n",
    "df_combined = pd.concat([df_test2, df_pred], axis=1)\n",
    "df_combined.to_csv('Submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction using LGBM\n",
    "y_prediction = best_model1.predict(df_test)\n",
    "df_pred = pd.DataFrame(y_prediction,columns=['Item_Outlet_Sales'])\n",
    "df_test2 = pd.read_csv('test.csv')\n",
    "df_test2 = df_test2[['Item_Identifier','Outlet_Identifier']]\n",
    "df_combined = pd.concat([df_test2, df_pred], axis=1)\n",
    "df_combined.to_csv('Submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbe141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction using RF and LGBM\n",
    "y_prediction1 = best_model.predict(df_test)\n",
    "y_prediction2 = best_model1.predict(df_test)\n",
    "df_pred = pd.DataFrame({\n",
    "    'Item_Outlet_Sales_RF': y_prediction1,\n",
    "    'Item_Outlet_Sales_LGB': y_prediction2\n",
    "})\n",
    "\n",
    "df_pred['Item_Outlet_Sales_LGB'] = np.where(df_pred['Item_Outlet_Sales_LGB'] < 0, df_pred['Item_Outlet_Sales_RF'], df_pred['Item_Outlet_Sales_LGB'])\n",
    "df_pred['Item_Outlet_Sales'] = 0.5*df_pred['Item_Outlet_Sales_LGB'] + 0.5*df_pred['Item_Outlet_Sales_RF']\n",
    "\n",
    "df_test2 = pd.read_csv('test.csv')\n",
    "df_combined = pd.concat([df_test2, df_pred], axis=1)\n",
    "df_combined = df_combined[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']]\n",
    "df_combined.to_csv('Submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328448e",
   "metadata": {},
   "source": [
    "Implement CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e013d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_label = ['Item_Type','Item_Identifier','Outlet_Location_Type','Outlet_Identifier','Outlet_Type']\n",
    "\n",
    "X = df_train1.drop('Item_Outlet_Sales', axis=1)\n",
    "y = df_train1['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e83b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.01, depth=6,\\\n",
    "#                          loss_function='RMSE', cat_features=list(features_label),nan_mode='Min')\n",
    "cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.01, depth=10,\\\n",
    "                          loss_function='RMSE', cat_features=list(features_label),nan_mode='Min')\n",
    "\n",
    "cat_model.fit(X_train, y_train, logging_level='Silent')\n",
    "\n",
    "y_pred = cat_model.predict(X_test)\n",
    "print(f'{cat_model} MSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = cat_model.predict(df_test1)\n",
    "df_pred = pd.DataFrame(y_prediction,columns=['Item_Outlet_Sales'])\n",
    "df_test2 = pd.read_csv('test.csv')\n",
    "df_test2 = df_test2[['Item_Identifier','Outlet_Identifier']]\n",
    "df_combined = pd.concat([df_test2, df_pred], axis=1)\n",
    "df_combined.to_csv('Submission4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
